defaults:
  - _self_
hydra:
  run:
    dir: ${log_dir}
_target_: src.agent.eval.EvalAgent

log_dir: ${oc.env:VLA_LOG_DIR}/eval_fractal/${name}_ta${act_steps}_${seed}/${env.task}_${now:%H-%M-%S}
name:
device: cuda
seed: 42
checkpoint_path:
n_eval_episode: ${eval:'3 * 4 * 9 * 10'}  # 3 drawers, 4 urdfs, 9 locations/rgb_overlay_paths, 10 trials each
n_video:  ${n_eval_episode}
# From Simpler: The robot is positioned in front of a cabinet that contains 3 drawers and instructed to open / close a specific drawer, testing its ability to manipulate articulated objects. We place the robot at 9 grid positions within a rectangle on the floor, yielding a total of 9 x 3 x 2 = 54 trials.
# The 9 locations are fixed w.r.t. rgb_overlay_path! https://github.com/simpler-env/ManiSkill2_real2sim/blob/87dc84508520310e61c972ece399a0f034095e42/mani_skill2_real2sim/envs/custom_scenes/open_drawer_in_scene.py#L210
# sweeps:
#   urdf_version:
#     - null
#     - "recolor_tabletop_visual_matching_1"
#     - "recolor_tabletop_visual_matching_2"
#     - "recolor_cabinet_visual_matching_1"
#   rgb_overlay_path:
#     - ./ManiSkill2_real2sim/data/real_inpainting/open_drawer_a0.png
#     - ./ManiSkill2_real2sim/data/real_inpainting/open_drawer_a1.png
#     - ./ManiSkill2_real2sim/data/real_inpainting/open_drawer_a2.png
#     - ./ManiSkill2_real2sim/data/real_inpainting/open_drawer_b0.png
#     - ./ManiSkill2_real2sim/data/real_inpainting/open_drawer_b1.png
#     - ./ManiSkill2_real2sim/data/real_inpainting/open_drawer_b2.png
#     - ./ManiSkill2_real2sim/data/real_inpainting/open_drawer_c0.png
#     - ./ManiSkill2_real2sim/data/real_inpainting/open_drawer_c1.png
#     - ./ManiSkill2_real2sim/data/real_inpainting/open_drawer_c2.png

env:
  task:
  adapter:
    _target_: src.agent.env_adapter.simpler.EDRSimplerAdapter
    dataset_statistics_path: config/fractal_statistics.json
    pretrained_model_path: ${oc.env:TRANSFORMERS_CACHE}/paligemma-3b-pt-224
    tokenizer_padding: max_length
    max_seq_len: 276  # fixed 256 for image + max 20 for text
    num_image_tokens: 256
    image_size: [224, 224]

flow_sampling: beta
num_inference_steps: 10
final_action_clip_value: 1.0  # data normalized in [-1,1]
use_torch_compile: True
use_bf16: False

cond_steps: 1
horizon_steps: 4
act_steps: 2
action_dim: 7 # EEF_POS
proprio_dim: 8  # POS_QUAT

mixture:
  vlm:   # gemma
    hidden_size: 2048
    intermediate_size: 16384
    use_final_norm: False
    cache: True
    use_quantize: ${quantize}
    use_lora: ${lora}
    adaptive_mode:  # not applicable for gemma
    rope_theta: 10000.0  # 10000 in gemma
  proprio:
    hidden_size: 1024
    intermediate_size: 4096
    use_final_norm: True  # technically no, but sharing mixture with action
    cache: True
    use_quantize: False
    use_lora: False
    adaptive_mode: ${action_expert_adaptive_mode}
    rope_theta: ${action_expert_rope_theta}
  action:
    hidden_size: 1024
    intermediate_size: 4096
    use_final_norm: True
    cache: False
    use_quantize: False
    use_lora: False
    adaptive_mode: ${action_expert_adaptive_mode}
    rope_theta: ${action_expert_rope_theta}
action_expert_adaptive_mode:  # adaLN, adaLN-Zero, or None
time_hidden_size: 256 # only applicable if using adaptive
time_max_period: 10000.0 # provided ckpts used 10000.0 for both time_max_period and action_expert_rope_theta
action_expert_rope_theta: 10000.0
quantize: False
lora: False
lora_r: 32
lora_dropout: 0.0
max_image_text_tokens: ${env.adapter.max_seq_len}

# Fixed
image_token_index: 257152
vocab_size: 257216
pad_token_id: 0

vision:
  _target_: src.model.paligemma.siglip.SiglipVisionModel
  config:
    hidden_size: 1152 # siglip
    intermediate_size: 4304
    num_hidden_layers: 27
    num_attention_heads: 16
    num_channels: 3
    image_size: 224
    patch_size: 14
    layer_norm_eps: 1e-6
    attention_dropout: 0.0
    num_image_tokens: 256

vision_projector:
  _target_: src.model.paligemma.siglip.PaliGemmaMultiModalProjector
  config:
    vision_config:
      hidden_size: 1152
      projection_dim: 2048

joint:
  _target_: src.model.vla.joint_model.JointModel
  config:
    action_expert_adaptive_mode: ${action_expert_adaptive_mode}
    time_hidden_size: ${time_hidden_size}
    mixture: ${mixture}
    lora:
      r: ${lora_r}
      dropout: ${lora_dropout}
    #
    num_hidden_layers: 18
    num_attention_heads: 8
    num_key_value_heads: 1
    head_dim: 256
    rms_norm_eps: 1e-6
    attention_bias: False
    attention_dropout: 0.0
    pad_token_id: ${pad_token_id}
